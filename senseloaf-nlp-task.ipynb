{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install stanza","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:48:59.625108Z","iopub.execute_input":"2022-06-04T11:48:59.625610Z","iopub.status.idle":"2022-06-04T11:48:59.629097Z","shell.execute_reply.started":"2022-06-04T11:48:59.625580Z","shell.execute_reply":"2022-06-04T11:48:59.628384Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport gensim\nfrom gensim.parsing.preprocessing import remove_stopwords\nimport re","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import stanza\nstanza.download('en')\nnlp = stanza.Pipeline(lang='en',use_gpu=True, processors='tokenize, lemma',pos_batch_size=3000)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:48:59.630335Z","iopub.execute_input":"2022-06-04T11:48:59.630834Z","iopub.status.idle":"2022-06-04T11:48:59.640756Z","shell.execute_reply.started":"2022-06-04T11:48:59.630802Z","shell.execute_reply":"2022-06-04T11:48:59.640063Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '../input/article-classification-assignment/dataset'","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:13:55.044420Z","iopub.execute_input":"2022-06-04T20:13:55.044841Z","iopub.status.idle":"2022-06-04T20:13:55.049895Z","shell.execute_reply.started":"2022-06-04T20:13:55.044810Z","shell.execute_reply":"2022-06-04T20:13:55.049039Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"files = os.listdir(DATA_DIR)\ndata = pd.DataFrame(columns=['text','clean_text'])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T20:13:55.920698Z","iopub.execute_input":"2022-06-04T20:13:55.921277Z","iopub.status.idle":"2022-06-04T20:13:55.935794Z","shell.execute_reply.started":"2022-06-04T20:13:55.921244Z","shell.execute_reply":"2022-06-04T20:13:55.934940Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for file_name in files:\n    file = f'{DATA_DIR}/{file_name}'\n    with open(file,'rb') as f:\n        contents = f.read().decode(errors='replace')\n        data = data.append({\n            'text': contents\n        },ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:49:00.360303Z","iopub.execute_input":"2022-06-04T11:49:00.361060Z","iopub.status.idle":"2022-06-04T11:49:00.365042Z","shell.execute_reply.started":"2022-06-04T11:49:00.361021Z","shell.execute_reply":"2022-06-04T11:49:00.364147Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def clean_document(text):\n    text = text = text.replace(\"\\'\",'\"').replace('\"',\"'\").replace('\\n',' ').strip()\n    return text\n\ndata['text'] = data['text'].apply(lambda x: clean_document(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:49:00.584768Z","iopub.execute_input":"2022-06-04T11:49:00.585184Z","iopub.status.idle":"2022-06-04T11:49:00.588864Z","shell.execute_reply.started":"2022-06-04T11:49:00.585150Z","shell.execute_reply":"2022-06-04T11:49:00.588139Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    doc = nlp(text)\n    lemmatized_sentence = ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n    cleansed_sentence = remove_stopwords(lemmatized_sentence)\n    clean_sentence = ' '.join(re.sub(r'[^\\w\\s]','',cleansed_sentence).split())\n    return clean_sentence\n\ndata['clean_text'] = data['text'].apply(lambda x: preprocess_text(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:49:01.236031Z","iopub.execute_input":"2022-06-04T11:49:01.237061Z","iopub.status.idle":"2022-06-04T11:49:01.240472Z","shell.execute_reply.started":"2022-06-04T11:49:01.236944Z","shell.execute_reply":"2022-06-04T11:49:01.239500Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Merged the original document and preprocessed document into one dataframe.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/preprocessedsl/final.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:49:01.890719Z","iopub.execute_input":"2022-06-04T11:49:01.893445Z","iopub.status.idle":"2022-06-04T11:49:02.153770Z","shell.execute_reply.started":"2022-06-04T11:49:01.893403Z","shell.execute_reply":"2022-06-04T11:49:02.152540Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def count_words(text):\n    text = text.split(' ')\n    return len(text)\n\ndef count_distinct_words(text):\n    text = text.split(' ')\n    text_set = set(text)\n    return len(text_set)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:49:02.155531Z","iopub.execute_input":"2022-06-04T11:49:02.156009Z","iopub.status.idle":"2022-06-04T11:49:02.162311Z","shell.execute_reply.started":"2022-06-04T11:49:02.155935Z","shell.execute_reply":"2022-06-04T11:49:02.161231Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df['original_len'] = df['text'].apply(lambda x: count_words(x))\ndf['processed_len'] = df['clean_text'].apply(lambda x: count_words(x))\ndf['original_unique_len'] = df['text'].apply(lambda x: count_distinct_words(x))\ndf['processed_unique_len'] = df['clean_text'].apply(lambda x: count_distinct_words(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:49:02.763047Z","iopub.execute_input":"2022-06-04T11:49:02.763833Z","iopub.status.idle":"2022-06-04T11:49:02.951473Z","shell.execute_reply.started":"2022-06-04T11:49:02.763797Z","shell.execute_reply":"2022-06-04T11:49:02.950464Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(df['clean_text'])\nX_normalized = preprocessing.normalize(X,norm='l2')\nX_adjusted = 2 - 2*cosine_similarity(X_normalized)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T11:49:04.213742Z","iopub.execute_input":"2022-06-04T11:49:04.214250Z","iopub.status.idle":"2022-06-04T11:49:05.005118Z","shell.execute_reply.started":"2022-06-04T11:49:04.214199Z","shell.execute_reply":"2022-06-04T11:49:05.004051Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"n_clusters = np.arange(5,21)\ninertia = []\nfor n_cluster in n_clusters:\n    model = KMeans(n_clusters=n_cluster)\n    model.fit(X)\n    inertia.append(model.inertia_)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T12:00:04.923650Z","iopub.execute_input":"2022-06-04T12:00:04.924078Z","iopub.status.idle":"2022-06-04T12:00:51.344529Z","shell.execute_reply.started":"2022-06-04T12:00:04.924044Z","shell.execute_reply":"2022-06-04T12:00:51.343785Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.arange(5,21),inertia)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T12:00:51.345940Z","iopub.execute_input":"2022-06-04T12:00:51.346472Z","iopub.status.idle":"2022-06-04T12:00:51.515034Z","shell.execute_reply.started":"2022-06-04T12:00:51.346437Z","shell.execute_reply":"2022-06-04T12:00:51.514330Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model = KMeans(n_clusters=15)\nmodel.fit(X)\ndf['label'] = model.labels_","metadata":{"execution":{"iopub.status.busy":"2022-06-04T12:05:59.844166Z","iopub.execute_input":"2022-06-04T12:05:59.844557Z","iopub.status.idle":"2022-06-04T12:06:02.733804Z","shell.execute_reply.started":"2022-06-04T12:05:59.844528Z","shell.execute_reply":"2022-06-04T12:06:02.732766Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"for num_cluster in np.arange(0,15):\n    data = df[df['label'] == num_cluster]\n    words = ''\n    text_list = [] \n    for texts in data['clean_text']:\n        text = texts.split()\n        for word in text:\n            if len(word) < 2:\n                text.remove(word)\n        text_list.extend(text)\n    text = ' '.join(text_list)\n    words += ''.join(text)\n           \n    wordcloud = WordCloud(width = 600, height = 600,\n                background_color ='white',\n                min_font_size = 10).generate(words)\n    print(f'---------------------Cluster Number: {num_cluster}-----------------------')\n    plt.figure(figsize = (6, 6), facecolor = None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 2)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T12:07:04.679880Z","iopub.execute_input":"2022-06-04T12:07:04.680276Z","iopub.status.idle":"2022-06-04T12:07:21.623004Z","shell.execute_reply.started":"2022-06-04T12:07:04.680245Z","shell.execute_reply":"2022-06-04T12:07:21.622287Z"},"trusted":true},"execution_count":46,"outputs":[]}]}